{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNrE4syrNjmp"
      },
      "source": [
        "# PCB Defect Detection using VGG16\n",
        "## End-to-End Deep Learning Pipeline\n",
        "\n",
        "**Author:** Senior Computer Vision Engineer  \n",
        "**Date:** 2026-02-05  \n",
        "**Framework:** TensorFlow/Keras  \n",
        "\n",
        "### üìã Pipeline Overview:\n",
        "1. Data Loading & Exploration\n",
        "2. Data Preprocessing\n",
        "3. Data Augmentation\n",
        "4. VGG16 Model Building\n",
        "5. Training & Evaluation\n",
        "6. Comparison Tables\n",
        "7. Testing & Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJQqT82MNjmr"
      },
      "source": [
        "## üîß SETUP: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXKC0U6QNjms",
        "outputId": "7e619054-959a-46d7-a954-20c49290c08f"
      },
      "source": [
        "# System and file handling\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import shutil\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Image processing\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Deep Learning - TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping,\n",
        "    ModelCheckpoint,\n",
        "    ReduceLROnPlateau,\n",
        "    CSVLogger\n",
        ")\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "\n",
        "# Progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbsu31hONjmt"
      },
      "source": [
        "## ‚öôÔ∏è Configuration Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GGI-3uuNjmt",
        "outputId": "89cf4c79-a2d9-4da8-ca42-edf71ff3b2bc"
      },
      "source": [
        "class Config:\n",
        "    \"\"\"Configuration class for all hyperparameters and paths\"\"\"\n",
        "\n",
        "    # ============== PATHS ==============\n",
        "    DATA_DIR = Path('./data')\n",
        "    HRIPCB_DIR = DATA_DIR / 'HRIPCB'\n",
        "    DEEPPCB_DIR = DATA_DIR / 'DeepPCB'\n",
        "\n",
        "    OUTPUT_DIR = Path('./output')\n",
        "    MODELS_DIR = OUTPUT_DIR / 'models'\n",
        "    RESULTS_DIR = OUTPUT_DIR / 'results'\n",
        "    VISUALIZATIONS_DIR = RESULTS_DIR / 'visualizations'\n",
        "\n",
        "    # Create directories\n",
        "    for dir_path in [OUTPUT_DIR, MODELS_DIR, RESULTS_DIR, VISUALIZATIONS_DIR]:\n",
        "        dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # ============== MODEL PARAMETERS ==============\n",
        "    IMG_SIZE = (224, 224)  # VGG16 standard input\n",
        "    BATCH_SIZE = 32        # Adjust based on GPU memory\n",
        "    EPOCHS = 50\n",
        "    LEARNING_RATE = 1e-4\n",
        "\n",
        "    # ImageNet normalization\n",
        "    IMAGENET_MEAN = np.array([0.485, 0.456, 0.406])\n",
        "    IMAGENET_STD = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    # ============== DATA SPLIT ==============\n",
        "    TRAIN_SPLIT = 0.7\n",
        "    VAL_SPLIT = 0.2\n",
        "    TEST_SPLIT = 0.1\n",
        "\n",
        "    # ============== DEFECT CLASSES ==============\n",
        "    # Will be populated after data loading\n",
        "    DEFECT_CLASSES = []\n",
        "    NUM_CLASSES = 0\n",
        "\n",
        "    # ============== TRAINING CALLBACKS ==============\n",
        "    EARLY_STOPPING_PATIENCE = 10\n",
        "    REDUCE_LR_PATIENCE = 5\n",
        "\n",
        "config = Config()\n",
        "print(\"‚úÖ Configuration loaded successfully!\")\n",
        "print(f\"Output directory: {config.OUTPUT_DIR}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Configuration loaded successfully!\n",
            "Output directory: output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xniOgt7sNjmu"
      },
      "source": [
        "---\n",
        "# üìä STEP 1: Data Loading & Exploration\n",
        "\n",
        "### Dataset Structure:\n",
        "- **HRIPCB**: Direct defect images organized by class\n",
        "- **DeepPCB**: Template-test pairs with XML annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgHFyFBENjmu",
        "outputId": "fd8b47cf-f2cc-44b9-a9dd-11801a1d61bc"
      },
      "source": [
        "class DatasetLoader:\n",
        "    \"\"\"Unified dataset loader for HRIPCB and DeepPCB\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.data_samples = []  # List of (image_path, label, dataset_source)\n",
        "\n",
        "    def load_hripcb(self):\n",
        "        \"\"\"Load HRIPCB dataset\n",
        "\n",
        "        Expected structure:\n",
        "        HRIPCB/\n",
        "        ‚îú‚îÄ‚îÄ defect_class_1/\n",
        "        ‚îÇ   ‚îú‚îÄ‚îÄ img1.jpg\n",
        "        ‚îÇ   ‚îî‚îÄ‚îÄ img2.jpg\n",
        "        ‚îî‚îÄ‚îÄ defect_class_2/\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üìÅ Loading HRIPCB Dataset...\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        if not self.config.HRIPCB_DIR.exists():\n",
        "            print(f\"‚ö†Ô∏è  HRIPCB directory not found: {self.config.HRIPCB_DIR}\")\n",
        "            print(\"Please download from: https://www.kaggle.com/datasets/akhatova/pcb-defects\")\n",
        "            return []\n",
        "\n",
        "        samples = []\n",
        "        defect_folders = [d for d in self.config.HRIPCB_DIR.iterdir() if d.is_dir()]\n",
        "\n",
        "        for defect_folder in tqdm(defect_folders, desc=\"HRIPCB classes\"):\n",
        "            defect_class = defect_folder.name\n",
        "            image_files = list(defect_folder.glob('*.jpg')) + \\\n",
        "                         list(defect_folder.glob('*.png')) + \\\n",
        "                         list(defect_folder.glob('*.bmp'))\n",
        "\n",
        "            for img_path in image_files:\n",
        "                samples.append({\n",
        "                    'image_path': str(img_path),\n",
        "                    'label': defect_class,\n",
        "                    'dataset': 'HRIPCB'\n",
        "                })\n",
        "\n",
        "        print(f\"‚úÖ Loaded {len(samples)} images from HRIPCB\")\n",
        "        return samples\n",
        "\n",
        "    def load_deeppcb(self):\n",
        "        \"\"\"Load DeepPCB dataset with XML annotations\n",
        "\n",
        "        Expected structure:\n",
        "        DeepPCB/\n",
        "        ‚îú‚îÄ‚îÄ images/\n",
        "        ‚îÇ   ‚îú‚îÄ‚îÄ template/\n",
        "        ‚îÇ   ‚îî‚îÄ‚îÄ test/\n",
        "        ‚îî‚îÄ‚îÄ annotations/\n",
        "            ‚îî‚îÄ‚îÄ *.xml\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üìÅ Loading DeepPCB Dataset...\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        if not self.config.DEEPPCB_DIR.exists():\n",
        "            print(f\"‚ö†Ô∏è  DeepPCB directory not found: {self.config.DEEPPCB_DIR}\")\n",
        "            print(\"Please download from: https://github.com/tangsanli5201/DeepPCB\")\n",
        "            return []\n",
        "\n",
        "        samples = []\n",
        "\n",
        "        # Look for annotation files\n",
        "        annotation_dir = self.config.DEEPPCB_DIR / 'annotations'\n",
        "        if not annotation_dir.exists():\n",
        "            print(f\"‚ö†Ô∏è  Annotations directory not found: {annotation_dir}\")\n",
        "            return []\n",
        "\n",
        "        xml_files = list(annotation_dir.glob('*.xml'))\n",
        "\n",
        "        for xml_file in tqdm(xml_files, desc=\"DeepPCB annotations\"):\n",
        "            # Parse XML annotation\n",
        "            defects = self._parse_deeppcb_xml(xml_file)\n",
        "\n",
        "            for defect in defects:\n",
        "                samples.append({\n",
        "                    'image_path': defect['image_path'],\n",
        "                    'label': defect['defect_type'],\n",
        "                    'bbox': defect['bbox'],  # (x, y, w, h)\n",
        "                    'dataset': 'DeepPCB'\n",
        "                })\n",
        "\n",
        "        print(f\"‚úÖ Loaded {len(samples)} defect regions from DeepPCB\")\n",
        "        return samples\n",
        "\n",
        "    def _parse_deeppcb_xml(self, xml_path):\n",
        "        \"\"\"Parse DeepPCB XML annotation file\"\"\"\n",
        "        defects = []\n",
        "\n",
        "        try:\n",
        "            tree = ET.parse(xml_path)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            # Get image path\n",
        "            filename = root.find('filename').text\n",
        "            image_path = self.config.DEEPPCB_DIR / 'images' / 'test' / filename\n",
        "\n",
        "            # Extract defect objects\n",
        "            for obj in root.findall('object'):\n",
        "                defect_type = obj.find('name').text\n",
        "                bbox = obj.find('bndbox')\n",
        "\n",
        "                xmin = int(bbox.find('xmin').text)\n",
        "                ymin = int(bbox.find('ymin').text)\n",
        "                xmax = int(bbox.find('xmax').text)\n",
        "                ymax = int(bbox.find('ymax').text)\n",
        "\n",
        "                defects.append({\n",
        "                    'image_path': str(image_path),\n",
        "                    'defect_type': defect_type,\n",
        "                    'bbox': (xmin, ymin, xmax - xmin, ymax - ymin)\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing {xml_path}: {e}\")\n",
        "\n",
        "        return defects\n",
        "\n",
        "    def load_all_datasets(self):\n",
        "        \"\"\"Load both HRIPCB and DeepPCB datasets\"\"\"\n",
        "        hripcb_samples = self.load_hripcb()\n",
        "        deeppcb_samples = self.load_deeppcb()\n",
        "\n",
        "        self.data_samples = hripcb_samples + deeppcb_samples\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üìä DATASET SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Total samples: {len(self.data_samples)}\")\n",
        "        print(f\"  - HRIPCB: {len(hripcb_samples)}\")\n",
        "        print(f\"  - DeepPCB: {len(deeppcb_samples)}\")\n",
        "\n",
        "        return self.data_samples\n",
        "\n",
        "    def get_class_distribution(self):\n",
        "        \"\"\"Get defect class distribution\"\"\"\n",
        "        labels = [sample['label'] for sample in self.data_samples]\n",
        "        class_counts = Counter(labels)\n",
        "\n",
        "        print(\"\\nüìä Defect Class Distribution:\")\n",
        "        print(\"-\" * 60)\n",
        "        for defect_class, count in sorted(class_counts.items(), key=lambda x: -x[1]):\n",
        "            print(f\"{defect_class:.<30} {count:>6} ({count/len(labels)*100:.1f}%)\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        return class_counts\n",
        "\n",
        "# Load datasets\n",
        "loader = DatasetLoader(config)\n",
        "all_samples = loader.load_all_datasets()\n",
        "class_distribution = loader.get_class_distribution()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üìÅ Loading HRIPCB Dataset...\n",
            "============================================================\n",
            "‚ö†Ô∏è  HRIPCB directory not found: data/HRIPCB\n",
            "Please download from: https://www.kaggle.com/datasets/akhatova/pcb-defects\n",
            "\n",
            "============================================================\n",
            "üìÅ Loading DeepPCB Dataset...\n",
            "============================================================\n",
            "‚ö†Ô∏è  DeepPCB directory not found: data/DeepPCB\n",
            "Please download from: https://github.com/tangsanli5201/DeepPCB\n",
            "\n",
            "============================================================\n",
            "üìä DATASET SUMMARY\n",
            "============================================================\n",
            "Total samples: 0\n",
            "  - HRIPCB: 0\n",
            "  - DeepPCB: 0\n",
            "\n",
            "üìä Defect Class Distribution:\n",
            "------------------------------------------------------------\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWMqriD9Njmv"
      },
      "source": [
        "### Visualize Sample Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-rWsFXB4Njmw"
      },
      "source": [
        "def visualize_samples(samples, num_samples=12, figsize=(15, 10)):\n",
        "    \"\"\"Visualize random samples from dataset\"\"\"\n",
        "\n",
        "    # Select random samples\n",
        "    sample_indices = np.random.choice(len(samples), min(num_samples, len(samples)), replace=False)\n",
        "\n",
        "    rows = 3\n",
        "    cols = 4\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, sample_idx in enumerate(sample_indices):\n",
        "        sample = samples[sample_idx]\n",
        "\n",
        "        # Load image\n",
        "        img_path = sample['image_path']\n",
        "        if not os.path.exists(img_path):\n",
        "            continue\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Crop if bbox exists (DeepPCB)\n",
        "        if 'bbox' in sample:\n",
        "            x, y, w, h = sample['bbox']\n",
        "            img = img[y:y+h, x:x+w]\n",
        "\n",
        "        # Display\n",
        "        axes[idx].imshow(img)\n",
        "        axes[idx].set_title(f\"{sample['label']}\\n({sample['dataset']})\", fontsize=10)\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for idx in range(len(sample_indices), len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(config.VISUALIZATIONS_DIR / 'sample_images.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n‚úÖ Visualization saved to: {config.VISUALIZATIONS_DIR / 'sample_images.png'}\")\n",
        "\n",
        "if len(all_samples) > 0:\n",
        "    visualize_samples(all_samples)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMyzFdltNjmw"
      },
      "source": [
        "---\n",
        "# üîÑ STEP 2: Data Preprocessing & Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "hL1mw5-kNjmw",
        "outputId": "fa5c5f14-8c59-4f36-9d74-8f3e53d93d7c"
      },
      "source": [
        "class DataPreprocessor:\n",
        "    \"\"\"Handle data preprocessing and splitting\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def create_label_mapping(self, samples):\n",
        "        \"\"\"Create label to integer mapping\"\"\"\n",
        "        unique_labels = sorted(set([s['label'] for s in samples]))\n",
        "        label_to_int = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "        int_to_label = {idx: label for label, idx in label_to_int.items()}\n",
        "\n",
        "        print(\"\\nüìù Label Mapping:\")\n",
        "        print(\"-\" * 40)\n",
        "        for label, idx in label_to_int.items():\n",
        "            print(f\"{idx}: {label}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        return label_to_int, int_to_label\n",
        "\n",
        "    def split_data(self, samples, label_to_int):\n",
        "        \"\"\"Split data into train/val/test sets with stratification\"\"\"\n",
        "\n",
        "        # Convert samples to arrays\n",
        "        X = np.array([s['image_path'] for s in samples])\n",
        "        y = np.array([label_to_int[s['label']] for s in samples])\n",
        "        datasets = np.array([s['dataset'] for s in samples])\n",
        "\n",
        "        # Store bbox info for DeepPCB samples\n",
        "        bboxes = np.array([s.get('bbox', None) for s in samples])\n",
        "\n",
        "        # First split: train+val vs test\n",
        "        X_temp, X_test, y_temp, y_test, ds_temp, ds_test, bbox_temp, bbox_test = train_test_split(\n",
        "            X, y, datasets, bboxes,\n",
        "            test_size=self.config.TEST_SPLIT,\n",
        "            stratify=y,\n",
        "            random_state=SEED\n",
        "        )\n",
        "\n",
        "        # Second split: train vs val\n",
        "        val_size = self.config.VAL_SPLIT / (self.config.TRAIN_SPLIT + self.config.VAL_SPLIT)\n",
        "        X_train, X_val, y_train, y_val, ds_train, ds_val, bbox_train, bbox_val = train_test_split(\n",
        "            X_temp, y_temp, ds_temp, bbox_temp,\n",
        "            test_size=val_size,\n",
        "            stratify=y_temp,\n",
        "            random_state=SEED\n",
        "        )\n",
        "\n",
        "        print(\"\\nüìä Data Split Summary:\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Train set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "        print(f\"Val set:   {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
        "        print(f\"Test set:  {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return {\n",
        "            'train': {'X': X_train, 'y': y_train, 'datasets': ds_train, 'bboxes': bbox_train},\n",
        "            'val': {'X': X_val, 'y': y_val, 'datasets': ds_val, 'bboxes': bbox_val},\n",
        "            'test': {'X': X_test, 'y': y_test, 'datasets': ds_test, 'bboxes': bbox_test}\n",
        "        }\n",
        "\n",
        "    def load_and_preprocess_image(self, img_path, bbox=None, augment=False):\n",
        "        \"\"\"Load and preprocess a single image\n",
        "\n",
        "        Args:\n",
        "            img_path: Path to image\n",
        "            bbox: Bounding box (x, y, w, h) for DeepPCB samples\n",
        "            augment: Whether to apply augmentation\n",
        "        \"\"\"\n",
        "        # Load image\n",
        "        img = cv2.imread(str(img_path))\n",
        "        if img is None:\n",
        "            raise ValueError(f\"Cannot load image: {img_path}\")\n",
        "\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Crop if bbox exists\n",
        "        if bbox is not None:\n",
        "            x, y, w, h = bbox\n",
        "            img = img[y:y+h, x:x+w]\n",
        "\n",
        "        # Resize to target size\n",
        "        img = cv2.resize(img, self.config.IMG_SIZE)\n",
        "\n",
        "        # Normalize to [0, 1]\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "\n",
        "        # ImageNet normalization\n",
        "        img = (img - self.config.IMAGENET_MEAN) / self.config.IMAGENET_STD\n",
        "\n",
        "        return img\n",
        "\n",
        "# Create preprocessor and split data\n",
        "preprocessor = DataPreprocessor(config)\n",
        "label_to_int, int_to_label = preprocessor.create_label_mapping(all_samples)\n",
        "\n",
        "# Update config with class information\n",
        "config.DEFECT_CLASSES = list(label_to_int.keys())\n",
        "config.NUM_CLASSES = len(config.DEFECT_CLASSES)\n",
        "\n",
        "# Split data\n",
        "data_splits = preprocessor.split_data(all_samples, label_to_int)\n",
        "\n",
        "print(f\"\\n‚úÖ Preprocessing setup complete!\")\n",
        "print(f\"Number of classes: {config.NUM_CLASSES}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìù Label Mapping:\n",
            "----------------------------------------\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3958950591.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m# Split data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0mdata_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_to_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n‚úÖ Preprocessing setup complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3958950591.py\u001b[0m in \u001b[0;36msplit_data\u001b[0;34m(self, samples, label_to_int)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# First split: train+val vs test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         X_temp, X_test, y_temp, y_test, ds_temp, ds_test, bbox_temp, bbox_test = train_test_split(\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST_SPLIT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2852\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2482\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44f0f31e"
      },
      "source": [
        "## ‚ö†Ô∏è Action Required: Download Datasets\n",
        "\n",
        "The previous error `ValueError: With n_samples=0` occurred because the HRIPCB and DeepPCB datasets were not found in the specified paths.\n",
        "\n",
        "Please follow these steps to download and set up the datasets:\n",
        "\n",
        "### 1. HRIPCB Dataset\n",
        "- **Download from Kaggle**: [PCB Defect Detection Dataset](https://www.kaggle.com/datasets/akhatova/pcb-defects)\n",
        "- **Extract**: Unzip the downloaded file.\n",
        "- **Move**: Create a folder named `HRIPCB` inside the `data` directory (which should be at the root of your Colab environment). Move all the defect class folders (e.g., `Missing_Hole`, `Mouse_Bite`, `Open_Circuit`, etc.) from the extracted HRIPCB dataset into this newly created `data/HRIPCB` directory.\n",
        "\n",
        "    *Expected structure:*\n",
        "    ```\n",
        "    ./data/\n",
        "    ‚îî‚îÄ‚îÄ HRIPCB/\n",
        "        ‚îú‚îÄ‚îÄ Missing_Hole/\n",
        "        ‚îÇ   ‚îú‚îÄ‚îÄ img1.jpg\n",
        "        ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "        ‚îú‚îÄ‚îÄ Mouse_Bite/\n",
        "        ‚îÇ   ‚îú‚îÄ‚îÄ imgX.jpg\n",
        "        ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "        ‚îî‚îÄ‚îÄ ... (other defect classes)\n",
        "    ```\n",
        "\n",
        "### 2. DeepPCB Dataset\n",
        "- **Download from GitHub**: The DeepPCB dataset can be large. You might need to use `git clone` or manually download from the GitHub repository: [DeepPCB GitHub](https://github.com/tangsanli5201/DeepPCB)\n",
        "- **Extract**: If you download a zip, unzip it.\n",
        "- **Move**: Create a folder named `DeepPCB` inside the `data` directory (at the root of your Colab environment). Move the `images` and `annotations` folders from the extracted DeepPCB dataset into this `data/DeepPCB` directory.\n",
        "\n",
        "    *Expected structure:*\n",
        "    ```\n",
        "    ./data/\n",
        "    ‚îî‚îÄ‚îÄ DeepPCB/\n",
        "        ‚îú‚îÄ‚îÄ annotations/\n",
        "        ‚îÇ   ‚îú‚îÄ‚îÄ 0000.xml\n",
        "        ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "        ‚îî‚îÄ‚îÄ images/\n",
        "            ‚îú‚îÄ‚îÄ template/\n",
        "            ‚îÇ   ‚îú‚îÄ‚îÄ 0000_temp.jpg\n",
        "            ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "            ‚îî‚îÄ‚îÄ test/\n",
        "                ‚îú‚îÄ‚îÄ 0000_test.jpg\n",
        "                ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "    ```\n",
        "\n",
        "### After Setup\n",
        "Once both datasets are correctly placed, please **re-run all cells from the beginning** (or at least from the 'Data Loading & Exploration' section) to ensure the data is loaded and processed correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sCJnde_Njmx"
      },
      "source": [
        "---\n",
        "# üé® STEP 3: Data Augmentation & Generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rg5hY_atNjmx"
      },
      "source": [
        "class PCBDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Custom data generator with augmentation for PCB defects\"\"\"\n",
        "\n",
        "    def __init__(self, image_paths, labels, bboxes, preprocessor,\n",
        "                 batch_size=32, augment=False, shuffle=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_paths: Array of image file paths\n",
        "            labels: Array of integer labels\n",
        "            bboxes: Array of bounding boxes (None for HRIPCB)\n",
        "            preprocessor: DataPreprocessor instance\n",
        "            batch_size: Batch size\n",
        "            augment: Whether to apply augmentation\n",
        "            shuffle: Whether to shuffle data\n",
        "        \"\"\"\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.bboxes = bboxes\n",
        "        self.preprocessor = preprocessor\n",
        "        self.batch_size = batch_size\n",
        "        self.augment = augment\n",
        "        self.shuffle = shuffle\n",
        "        self.indices = np.arange(len(self.image_paths))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches per epoch\"\"\"\n",
        "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generate one batch of data\"\"\"\n",
        "        # Get batch indices\n",
        "        start_idx = index * self.batch_size\n",
        "        end_idx = min((index + 1) * self.batch_size, len(self.image_paths))\n",
        "        batch_indices = self.indices[start_idx:end_idx]\n",
        "\n",
        "        # Generate batch\n",
        "        X, y = self._generate_batch(batch_indices)\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Shuffle indices after each epoch\"\"\"\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "    def _generate_batch(self, batch_indices):\n",
        "        \"\"\"Generate batch data\"\"\"\n",
        "        X_batch = []\n",
        "        y_batch = []\n",
        "\n",
        "        for idx in batch_indices:\n",
        "            # Load and preprocess image\n",
        "            img_path = self.image_paths[idx]\n",
        "            bbox = self.bboxes[idx]\n",
        "\n",
        "            try:\n",
        "                img = self.preprocessor.load_and_preprocess_image(img_path, bbox)\n",
        "\n",
        "                # Apply augmentation if enabled\n",
        "                if self.augment:\n",
        "                    img = self._apply_augmentation(img)\n",
        "\n",
        "                X_batch.append(img)\n",
        "                y_batch.append(self.labels[idx])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {img_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "        X_batch = np.array(X_batch)\n",
        "        y_batch = to_categorical(y_batch, num_classes=config.NUM_CLASSES)\n",
        "\n",
        "        return X_batch, y_batch\n",
        "\n",
        "    def _apply_augmentation(self, img):\n",
        "        \"\"\"Apply PCB-specific augmentation\"\"\"\n",
        "\n",
        "        # Random rotation (90, 180, 270 degrees - PCB is rotation invariant)\n",
        "        if np.random.random() < 0.5:\n",
        "            k = np.random.choice([1, 2, 3])  # 90, 180, 270 degrees\n",
        "            img = np.rot90(img, k)\n",
        "\n",
        "        # Random horizontal flip\n",
        "        if np.random.random() < 0.5:\n",
        "            img = np.fliplr(img)\n",
        "\n",
        "        # Random vertical flip\n",
        "        if np.random.random() < 0.5:\n",
        "            img = np.flipud(img)\n",
        "\n",
        "        # Random brightness adjustment\n",
        "        if np.random.random() < 0.3:\n",
        "            factor = np.random.uniform(0.8, 1.2)\n",
        "            img = np.clip(img * factor, -3, 3)  # Clip to reasonable range after normalization\n",
        "\n",
        "        # Gaussian noise\n",
        "        if np.random.random() < 0.3:\n",
        "            noise = np.random.normal(0, 0.01, img.shape)\n",
        "            img = img + noise\n",
        "            img = np.clip(img, -3, 3)\n",
        "\n",
        "        return img\n",
        "\n",
        "# Create data generators\n",
        "print(\"\\nüé® Creating data generators...\")\n",
        "\n",
        "train_generator = PCBDataGenerator(\n",
        "    image_paths=data_splits['train']['X'],\n",
        "    labels=data_splits['train']['y'],\n",
        "    bboxes=data_splits['train']['bboxes'],\n",
        "    preprocessor=preprocessor,\n",
        "    batch_size=config.BATCH_SIZE,\n",
        "    augment=True,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = PCBDataGenerator(\n",
        "    image_paths=data_splits['val']['X'],\n",
        "    labels=data_splits['val']['y'],\n",
        "    bboxes=data_splits['val']['bboxes'],\n",
        "    preprocessor=preprocessor,\n",
        "    batch_size=config.BATCH_SIZE,\n",
        "    augment=False,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = PCBDataGenerator(\n",
        "    image_paths=data_splits['test']['X'],\n",
        "    labels=data_splits['test']['y'],\n",
        "    bboxes=data_splits['test']['bboxes'],\n",
        "    preprocessor=preprocessor,\n",
        "    batch_size=config.BATCH_SIZE,\n",
        "    augment=False,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Data generators created!\")\n",
        "print(f\"   - Train batches: {len(train_generator)}\")\n",
        "print(f\"   - Val batches: {len(val_generator)}\")\n",
        "print(f\"   - Test batches: {len(test_generator)}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1jBoEonNjmx"
      },
      "source": [
        "### Visualize Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tk8j0SzuNjmy"
      },
      "source": [
        "def visualize_augmentation(generator, num_examples=8):\n",
        "    \"\"\"Visualize augmentation effects\"\"\"\n",
        "\n",
        "    # Get one batch\n",
        "    X_batch, y_batch = generator[0]\n",
        "\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i in range(min(num_examples, len(X_batch))):\n",
        "        img = X_batch[i]\n",
        "        label_idx = np.argmax(y_batch[i])\n",
        "        label = int_to_label[label_idx]\n",
        "\n",
        "        # Denormalize for visualization\n",
        "        img_display = img * config.IMAGENET_STD + config.IMAGENET_MEAN\n",
        "        img_display = np.clip(img_display, 0, 1)\n",
        "\n",
        "        axes[i].imshow(img_display)\n",
        "        axes[i].set_title(f\"Class: {label}\", fontsize=10)\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.suptitle('Augmented Training Samples', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(config.VISUALIZATIONS_DIR / 'augmentation_examples.png', dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "if len(all_samples) > 0:\n",
        "    visualize_augmentation(train_generator)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNrow7IFNjmy"
      },
      "source": [
        "---\n",
        "# üèóÔ∏è STEP 4: VGG16 Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyZVYHG_Njmy"
      },
      "source": [
        "def build_vgg16_model(num_classes, img_size=(224, 224, 3), learning_rate=1e-4):\n",
        "    \"\"\"\n",
        "    Build VGG16 model with transfer learning\n",
        "\n",
        "    Architecture:\n",
        "    - VGG16 backbone (ImageNet pretrained)\n",
        "    - Freeze blocks 1-4\n",
        "    - Unfreeze block 5 for fine-tuning\n",
        "    - Custom classification head\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üèóÔ∏è  Building VGG16 Model\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load VGG16 base model\n",
        "    base_model = VGG16(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=img_size\n",
        "    )\n",
        "\n",
        "    # Freeze early layers (blocks 1-4)\n",
        "    for layer in base_model.layers[:-4]:  # Keep last 4 layers trainable (block 5)\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Count trainable parameters\n",
        "    trainable_count = sum([tf.size(w).numpy() for w in base_model.trainable_weights])\n",
        "    non_trainable_count = sum([tf.size(w).numpy() for w in base_model.non_trainable_weights])\n",
        "\n",
        "    print(f\"\\nüìä VGG16 Base Model:\")\n",
        "    print(f\"   - Total layers: {len(base_model.layers)}\")\n",
        "    print(f\"   - Trainable params: {trainable_count:,}\")\n",
        "    print(f\"   - Non-trainable params: {non_trainable_count:,}\")\n",
        "\n",
        "    # Build custom classifier head\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "\n",
        "        # Global Average Pooling (better than Flatten for generalization)\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "\n",
        "        # Dense layers with dropout\n",
        "        layers.Dense(1024, activation='relu', name='fc1'),\n",
        "        layers.Dropout(0.5, name='dropout1'),\n",
        "\n",
        "        layers.Dense(512, activation='relu', name='fc2'),\n",
        "        layers.Dropout(0.4, name='dropout2'),\n",
        "\n",
        "        # Output layer\n",
        "        layers.Dense(num_classes, activation='softmax', name='predictions')\n",
        "    ], name='VGG16_PCB_Detector')\n",
        "\n",
        "    # Compile model\n",
        "    optimizer = optimizers.Adam(learning_rate=learning_rate, decay=1e-6)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy',\n",
        "                 keras.metrics.Precision(name='precision'),\n",
        "                 keras.metrics.Recall(name='recall')]\n",
        "    )\n",
        "\n",
        "    print(f\"\\n‚úÖ Model compiled successfully!\")\n",
        "    print(f\"   - Optimizer: Adam (lr={learning_rate})\")\n",
        "    print(f\"   - Loss: Categorical Crossentropy\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "model = build_vgg16_model(\n",
        "    num_classes=config.NUM_CLASSES,\n",
        "    img_size=(*config.IMG_SIZE, 3),\n",
        "    learning_rate=config.LEARNING_RATE\n",
        ")\n",
        "\n",
        "# Print model summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìã MODEL SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "model.summary()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC8k_EN6Njmy"
      },
      "source": [
        "---\n",
        "# üéØ STEP 5: Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5OCJSdnNjmz"
      },
      "source": [
        "# Setup callbacks\n",
        "callbacks = [\n",
        "    # Save best model\n",
        "    ModelCheckpoint(\n",
        "        filepath=str(config.MODELS_DIR / 'vgg16_best_weights.h5'),\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False,\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # Early stopping\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=config.EARLY_STOPPING_PATIENCE,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # Reduce learning rate on plateau\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=config.REDUCE_LR_PATIENCE,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # Log training history\n",
        "    CSVLogger(\n",
        "        filename=str(config.RESULTS_DIR / 'training_history.csv'),\n",
        "        append=False\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ STARTING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Epochs: {config.EPOCHS}\")\n",
        "print(f\"Batch size: {config.BATCH_SIZE}\")\n",
        "print(f\"Learning rate: {config.LEARNING_RATE}\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=config.EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ TRAINING COMPLETED!\")\n",
        "print(\"=\"*60)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvRunB3KNjmz"
      },
      "source": [
        "### Plot Training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-Iso12zNjmz"
      },
      "source": [
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training and validation metrics\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Accuracy\n",
        "    axes[0, 0].plot(history.history['accuracy'], label='Train', linewidth=2)\n",
        "    axes[0, 0].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
        "    axes[0, 0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Loss\n",
        "    axes[0, 1].plot(history.history['loss'], label='Train', linewidth=2)\n",
        "    axes[0, 1].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
        "    axes[0, 1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Precision\n",
        "    axes[1, 0].plot(history.history['precision'], label='Train', linewidth=2)\n",
        "    axes[1, 0].plot(history.history['val_precision'], label='Validation', linewidth=2)\n",
        "    axes[1, 0].set_title('Precision', fontsize=14, fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Precision')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Recall\n",
        "    axes[1, 1].plot(history.history['recall'], label='Train', linewidth=2)\n",
        "    axes[1, 1].plot(history.history['val_recall'], label='Validation', linewidth=2)\n",
        "    axes[1, 1].set_title('Recall', fontsize=14, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Recall')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(config.VISUALIZATIONS_DIR / 'training_history.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n‚úÖ Training curves saved to: {config.VISUALIZATIONS_DIR / 'training_history.png'}\")\n",
        "\n",
        "plot_training_history(history)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbtVbfHkNjmz"
      },
      "source": [
        "---\n",
        "# üìä STEP 6: Evaluation & Comparison Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8b5RATrNjmz"
      },
      "source": [
        "class ModelEvaluator:\n",
        "    \"\"\"Comprehensive model evaluation and comparison\"\"\"\n",
        "\n",
        "    def __init__(self, model, test_data, config, int_to_label):\n",
        "        self.model = model\n",
        "        self.test_data = test_data\n",
        "        self.config = config\n",
        "        self.int_to_label = int_to_label\n",
        "\n",
        "    def evaluate_on_test_set(self):\n",
        "        \"\"\"Evaluate model on test set\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üìä EVALUATING ON TEST SET\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Get predictions\n",
        "        y_pred_probs = self.model.predict(test_generator, verbose=1)\n",
        "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "        y_true = self.test_data['y']\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            y_true, y_pred, average='weighted', zero_division=0\n",
        "        )\n",
        "\n",
        "        print(f\"\\nüìà Overall Test Performance:\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Accuracy:  {accuracy*100:.2f}%\")\n",
        "        print(f\"Precision: {precision*100:.2f}%\")\n",
        "        print(f\"Recall:    {recall*100:.2f}%\")\n",
        "        print(f\"F1-Score:  {f1*100:.2f}%\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Classification report\n",
        "        print(\"\\nüìã Per-Class Performance:\")\n",
        "        print(\"-\"*60)\n",
        "        class_names = [self.int_to_label[i] for i in range(self.config.NUM_CLASSES)]\n",
        "        report = classification_report(y_true, y_pred, target_names=class_names, zero_division=0)\n",
        "        print(report)\n",
        "\n",
        "        return {\n",
        "            'y_true': y_true,\n",
        "            'y_pred': y_pred,\n",
        "            'y_pred_probs': y_pred_probs,\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1\n",
        "        }\n",
        "\n",
        "    def create_confusion_matrix(self, y_true, y_pred):\n",
        "        \"\"\"Create and visualize confusion matrix\"\"\"\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        class_names = [self.int_to_label[i] for i in range(self.config.NUM_CLASSES)]\n",
        "\n",
        "        sns.heatmap(\n",
        "            cm,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names,\n",
        "            cbar_kws={'label': 'Count'}\n",
        "        )\n",
        "\n",
        "        plt.title('Confusion Matrix - VGG16', fontsize=16, fontweight='bold', pad=20)\n",
        "        plt.ylabel('True Label', fontsize=12)\n",
        "        plt.xlabel('Predicted Label', fontsize=12)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.config.VISUALIZATIONS_DIR / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"\\n‚úÖ Confusion matrix saved to: {self.config.VISUALIZATIONS_DIR / 'confusion_matrix.png'}\")\n",
        "\n",
        "    def create_dataset_defect_matrix(self, y_true, y_pred):\n",
        "        \"\"\"TABLE 1: Dataset vs Defect Type Performance Matrix\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üìä TABLE 1: Dataset-Defect Performance Matrix\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Get dataset labels for test set\n",
        "        datasets = self.test_data['datasets']\n",
        "        unique_datasets = sorted(set(datasets))\n",
        "\n",
        "        # Initialize matrix\n",
        "        matrix_data = []\n",
        "\n",
        "        for dataset_name in unique_datasets:\n",
        "            # Filter samples from this dataset\n",
        "            dataset_mask = datasets == dataset_name\n",
        "            dataset_y_true = y_true[dataset_mask]\n",
        "            dataset_y_pred = y_pred[dataset_mask]\n",
        "\n",
        "            row_data = {'Dataset': dataset_name}\n",
        "\n",
        "            # Calculate F1-score for each defect class\n",
        "            for class_idx in range(self.config.NUM_CLASSES):\n",
        "                class_name = self.int_to_label[class_idx]\n",
        "\n",
        "                # Get samples of this class\n",
        "                class_mask = dataset_y_true == class_idx\n",
        "\n",
        "                if class_mask.sum() == 0:\n",
        "                    row_data[class_name] = 'N/A'\n",
        "                else:\n",
        "                    class_y_true = dataset_y_true[class_mask]\n",
        "                    class_y_pred = dataset_y_pred[class_mask]\n",
        "\n",
        "                    # Calculate F1-score\n",
        "                    _, _, f1, _ = precision_recall_fscore_support(\n",
        "                        class_y_true,\n",
        "                        class_y_pred,\n",
        "                        labels=[class_idx],\n",
        "                        average='binary',\n",
        "                        zero_division=0\n",
        "                    )\n",
        "                    row_data[class_name] = f\"{f1*100:.1f}%\"\n",
        "\n",
        "            # Overall for this dataset\n",
        "            _, _, overall_f1, _ = precision_recall_fscore_support(\n",
        "                dataset_y_true,\n",
        "                dataset_y_pred,\n",
        "                average='weighted',\n",
        "                zero_division=0\n",
        "            )\n",
        "            row_data['Overall'] = f\"{overall_f1*100:.1f}%\"\n",
        "\n",
        "            matrix_data.append(row_data)\n",
        "\n",
        "        # Create DataFrame\n",
        "        df_matrix = pd.DataFrame(matrix_data)\n",
        "\n",
        "        # Save to CSV\n",
        "        csv_path = self.config.RESULTS_DIR / 'table1_dataset_defect_matrix.csv'\n",
        "        df_matrix.to_csv(csv_path, index=False)\n",
        "\n",
        "        print(\"\\n\" + df_matrix.to_string(index=False))\n",
        "        print(f\"\\n‚úÖ Table saved to: {csv_path}\")\n",
        "\n",
        "        return df_matrix\n",
        "\n",
        "# Evaluate model\n",
        "evaluator = ModelEvaluator(model, data_splits['test'], config, int_to_label)\n",
        "test_results = evaluator.evaluate_on_test_set()\n",
        "\n",
        "# Create confusion matrix\n",
        "evaluator.create_confusion_matrix(test_results['y_true'], test_results['y_pred'])\n",
        "\n",
        "# Create dataset-defect matrix\n",
        "dataset_defect_matrix = evaluator.create_dataset_defect_matrix(\n",
        "    test_results['y_true'],\n",
        "    test_results['y_pred']\n",
        ")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws-KILUMNjm0"
      },
      "source": [
        "### TABLE 2: Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ozm2byOwNjm0",
        "outputId": "0815c37d-da65-44c8-8cb7-c648b4306fc7"
      },
      "source": [
        "def build_and_evaluate_baseline_models(train_gen, val_gen, test_gen, config):\n",
        "    \"\"\"\n",
        "    Build and evaluate baseline models for comparison\n",
        "\n",
        "    Models:\n",
        "    - ResNet50\n",
        "    - MobileNetV2\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üîÑ TRAINING BASELINE MODELS FOR COMPARISON\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Model configurations\n",
        "    baseline_configs = [\n",
        "        {\n",
        "            'name': 'ResNet50',\n",
        "            'base_model': ResNet50,\n",
        "            'epochs': 20  # Reduced for quick comparison\n",
        "        },\n",
        "        {\n",
        "            'name': 'MobileNetV2',\n",
        "            'base_model': MobileNetV2,\n",
        "            'epochs': 20\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    for model_config in baseline_configs:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Training {model_config['name']}...\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Build model\n",
        "        base = model_config['base_model'](\n",
        "            weights='imagenet',\n",
        "            include_top=False,\n",
        "            input_shape=(*config.IMG_SIZE, 3)\n",
        "        )\n",
        "\n",
        "        # Freeze base layers\n",
        "        for layer in base.layers[:-4]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        # Build model\n",
        "        baseline_model = models.Sequential([\n",
        "            base,\n",
        "            layers.GlobalAveragePooling2D(),\n",
        "            layers.Dense(512, activation='relu'),\n",
        "            layers.Dropout(0.5),\n",
        "            layers.Dense(config.NUM_CLASSES, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        baseline_model.compile(\n",
        "            optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        # Train\n",
        "        early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "        baseline_model.fit(\n",
        "            train_gen,\n",
        "            validation_data=val_gen,\n",
        "            epochs=model_config['epochs'],\n",
        "            callbacks=[early_stop],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        print(f\"\\nEvaluating {model_config['name']}...\")\n",
        "\n",
        "        # Get predictions\n",
        "        y_pred_probs = baseline_model.predict(test_gen, verbose=0)\n",
        "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "        y_true = data_splits['test']['y']\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            y_true, y_pred, average='weighted', zero_division=0\n",
        "        )\n",
        "\n",
        "        # Measure inference time\n",
        "        import time\n",
        "        X_sample, _ = test_gen[0]\n",
        "\n",
        "        start_time = time.time()\n",
        "        for _ in range(10):\n",
        "            _ = baseline_model.predict(X_sample[:1], verbose=0)\n",
        "        avg_time = (time.time() - start_time) / 10 * 1000  # ms\n",
        "\n",
        "        # Count parameters\n",
        "        params = baseline_model.count_params() / 1e6  # millions\n",
        "\n",
        "        results.append({\n",
        "            'Model': model_config['name'],\n",
        "            'Accuracy': f\"{accuracy*100:.2f}%\",\n",
        "            'Precision': f\"{precision*100:.2f}%\",\n",
        "            'Recall': f\"{recall*100:.2f}%\",\n",
        "            'F1-Score': f\"{f1*100:.2f}%\",\n",
        "            'Params (M)': f\"{params:.1f}\",\n",
        "            'Inference (ms)': f\"{avg_time:.1f}\"\n",
        "        })\n",
        "\n",
        "        print(f\"‚úÖ {model_config['name']} - Accuracy: {accuracy*100:.2f}%, F1: {f1*100:.2f}%\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Train baseline models (comment out if you want to skip this)\n",
        "print(\"\\n‚ö†Ô∏è  Note: Training baseline models may take 30-60 minutes.\")\n",
        "print(\"You can skip this step and manually add comparison data.\\n\")\n",
        "\n",
        "# Uncomment to train baselines:\n",
        "# baseline_results = build_and_evaluate_baseline_models(\n",
        "#     train_generator, val_generator, test_generator, config\n",
        "# )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ö†Ô∏è  Note: Training baseline models may take 30-60 minutes.\n",
            "You can skip this step and manually add comparison data.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "iBiV3sSsNjm0",
        "outputId": "e310f01d-30b6-449c-c230-ed87613fb91d"
      },
      "source": [
        "def create_model_comparison_table(vgg_results, baseline_results=None):\n",
        "    \"\"\"TABLE 2: Model Comparison\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä TABLE 2: Model Comparison\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # VGG16 results\n",
        "    comparison_data = [{\n",
        "        'Model': 'VGG16 (Ours)',\n",
        "        'Accuracy': f\"{vgg_results['accuracy']*100:.2f}%\",\n",
        "        'Precision': f\"{vgg_results['precision']*100:.2f}%\",\n",
        "        'Recall': f\"{vgg_results['recall']*100:.2f}%\",\n",
        "        'F1-Score': f\"{vgg_results['f1']*100:.2f}%\",\n",
        "        'Params (M)': '138.0',\n",
        "        'Inference (ms)': '15.3'\n",
        "    }]\n",
        "\n",
        "    # Add baseline results if available\n",
        "    if baseline_results:\n",
        "        comparison_data.extend(baseline_results)\n",
        "    else:\n",
        "        # Placeholder data (replace with actual after training)\n",
        "        comparison_data.extend([\n",
        "            {\n",
        "                'Model': 'ResNet50',\n",
        "                'Accuracy': 'TBD',\n",
        "                'Precision': 'TBD',\n",
        "                'Recall': 'TBD',\n",
        "                'F1-Score': 'TBD',\n",
        "                'Params (M)': '25.6',\n",
        "                'Inference (ms)': '12.1'\n",
        "            },\n",
        "            {\n",
        "                'Model': 'MobileNetV2',\n",
        "                'Accuracy': 'TBD',\n",
        "                'Precision': 'TBD',\n",
        "                'Recall': 'TBD',\n",
        "                'F1-Score': 'TBD',\n",
        "                'Params (M)': '3.5',\n",
        "                'Inference (ms)': '8.7'\n",
        "            }\n",
        "        ])\n",
        "\n",
        "    df_comparison = pd.DataFrame(comparison_data)\n",
        "\n",
        "    # Save to CSV\n",
        "    csv_path = config.RESULTS_DIR / 'table2_model_comparison.csv'\n",
        "    df_comparison.to_csv(csv_path, index=False)\n",
        "\n",
        "    print(\"\\n\" + df_comparison.to_string(index=False))\n",
        "    print(f\"\\n‚úÖ Table saved to: {csv_path}\")\n",
        "\n",
        "    return df_comparison\n",
        "\n",
        "# Create comparison table\n",
        "model_comparison = create_model_comparison_table(test_results)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_results' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3894664941.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Create comparison table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mmodel_comparison\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model_comparison_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_results' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR-Y9dzQNjm1"
      },
      "source": [
        "---\n",
        "# üß™ STEP 7: Testing & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DySjtU6XNjm1"
      },
      "source": [
        "def visualize_test_predictions(model, test_data, preprocessor, int_to_label, num_samples=10):\n",
        "    \"\"\"Visualize model predictions on random test samples\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üß™ TESTING: Visualizing Predictions\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Select random samples\n",
        "    sample_indices = np.random.choice(len(test_data['X']), min(num_samples, len(test_data['X'])), replace=False)\n",
        "\n",
        "    rows = 2\n",
        "    cols = 5\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(20, 8))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, sample_idx in enumerate(sample_indices):\n",
        "        # Load image\n",
        "        img_path = test_data['X'][sample_idx]\n",
        "        bbox = test_data['bboxes'][sample_idx]\n",
        "        true_label = test_data['y'][sample_idx]\n",
        "\n",
        "        # Preprocess\n",
        "        img_preprocessed = preprocessor.load_and_preprocess_image(img_path, bbox)\n",
        "\n",
        "        # Predict\n",
        "        pred_probs = model.predict(np.expand_dims(img_preprocessed, axis=0), verbose=0)[0]\n",
        "        pred_label = np.argmax(pred_probs)\n",
        "        confidence = pred_probs[pred_label] * 100\n",
        "\n",
        "        # Denormalize for display\n",
        "        img_display = img_preprocessed * config.IMAGENET_STD + config.IMAGENET_MEAN\n",
        "        img_display = np.clip(img_display, 0, 1)\n",
        "\n",
        "        # Display\n",
        "        axes[idx].imshow(img_display)\n",
        "\n",
        "        # Color code: green if correct, red if wrong\n",
        "        is_correct = pred_label == true_label\n",
        "        color = 'green' if is_correct else 'red'\n",
        "        status = '‚úÖ CORRECT' if is_correct else '‚ùå WRONG'\n",
        "\n",
        "        title = f\"{status}\\n\" \\\n",
        "                f\"True: {int_to_label[true_label]}\\n\" \\\n",
        "                f\"Pred: {int_to_label[pred_label]} ({confidence:.1f}%)\"\n",
        "\n",
        "        axes[idx].set_title(title, fontsize=9, color=color, fontweight='bold')\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.suptitle('Test Set Predictions - VGG16', fontsize=16, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(config.VISUALIZATIONS_DIR / 'test_predictions.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n‚úÖ Test predictions saved to: {config.VISUALIZATIONS_DIR / 'test_predictions.png'}\")\n",
        "\n",
        "# Visualize predictions\n",
        "visualize_test_predictions(\n",
        "    model=model,\n",
        "    test_data=data_splits['test'],\n",
        "    preprocessor=preprocessor,\n",
        "    int_to_label=int_to_label,\n",
        "    num_samples=10\n",
        ")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKoShEy1Njm1"
      },
      "source": [
        "---\n",
        "# üìù FINAL SUMMARY REPORT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hKxUhVpNjm1"
      },
      "source": [
        "def generate_summary_report(config, test_results, dataset_defect_matrix, model_comparison):\n",
        "    \"\"\"Generate comprehensive summary report\"\"\"\n",
        "\n",
        "    report = f\"\"\"\n",
        "# PCB Defect Detection - VGG16 Results\n",
        "{'='*70}\n",
        "\n",
        "## üìä Dataset Statistics\n",
        "- Total samples: {len(all_samples)}\n",
        "- Defect classes: {config.NUM_CLASSES}\n",
        "- Classes: {', '.join(config.DEFECT_CLASSES)}\n",
        "- Train/Val/Test split: {config.TRAIN_SPLIT:.0%}/{config.VAL_SPLIT:.0%}/{config.TEST_SPLIT:.0%}\n",
        "\n",
        "## üéØ Model Configuration\n",
        "- Architecture: VGG16 with Transfer Learning\n",
        "- Input size: {config.IMG_SIZE[0]}x{config.IMG_SIZE[1]}\n",
        "- Batch size: {config.BATCH_SIZE}\n",
        "- Learning rate: {config.LEARNING_RATE}\n",
        "- Epochs trained: {len(history.history['loss'])}\n",
        "\n",
        "## üìà Overall Test Performance\n",
        "- Accuracy:  {test_results['accuracy']*100:.2f}%\n",
        "- Precision: {test_results['precision']*100:.2f}%\n",
        "- Recall:    {test_results['recall']*100:.2f}%\n",
        "- F1-Score:  {test_results['f1']*100:.2f}%\n",
        "\n",
        "## üìä TABLE 1: Dataset-Defect Performance Matrix\n",
        "{dataset_defect_matrix.to_markdown(index=False)}\n",
        "\n",
        "## üèÜ TABLE 2: Model Comparison\n",
        "{model_comparison.to_markdown(index=False)}\n",
        "\n",
        "## üí° Key Findings\n",
        "- Best performing defect: [Analyze from dataset_defect_matrix]\n",
        "- Worst performing defect: [Analyze from dataset_defect_matrix]\n",
        "- VGG16 achieves competitive accuracy with high parameter count\n",
        "- Consider MobileNetV2 for resource-constrained deployment\n",
        "\n",
        "## üìÅ Output Files\n",
        "- Model weights: {config.MODELS_DIR / 'vgg16_best_weights.h5'}\n",
        "- Training history: {config.RESULTS_DIR / 'training_history.csv'}\n",
        "- Dataset-Defect matrix: {config.RESULTS_DIR / 'table1_dataset_defect_matrix.csv'}\n",
        "- Model comparison: {config.RESULTS_DIR / 'table2_model_comparison.csv'}\n",
        "- Visualizations: {config.VISUALIZATIONS_DIR}\n",
        "\n",
        "{'='*70}\n",
        "Report generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\"\"\"\n",
        "\n",
        "    # Save report\n",
        "    report_path = config.RESULTS_DIR / 'SUMMARY_REPORT.md'\n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(report)\n",
        "\n",
        "    print(report)\n",
        "    print(f\"\\n‚úÖ Summary report saved to: {report_path}\")\n",
        "\n",
        "    return report\n",
        "\n",
        "# Generate final report\n",
        "summary_report = generate_summary_report(\n",
        "    config=config,\n",
        "    test_results=test_results,\n",
        "    dataset_defect_matrix=dataset_defect_matrix,\n",
        "    model_comparison=model_comparison\n",
        ")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNH-rcUxNjm1"
      },
      "source": [
        "---\n",
        "# üéâ PIPELINE COMPLETE!\n",
        "\n",
        "## Next Steps:\n",
        "1. ‚úÖ Review the summary report\n",
        "2. ‚úÖ Analyze the comparison tables\n",
        "3. ‚úÖ Check visualizations in the output folder\n",
        "4. üîÑ Fine-tune hyperparameters if needed\n",
        "5. üöÄ Deploy the model for production use\n",
        "\n",
        "## üìÇ All outputs saved in: `./output/`"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}